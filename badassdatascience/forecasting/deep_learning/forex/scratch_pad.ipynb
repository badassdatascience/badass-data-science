{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a696865-0400-4a18-a8dc-8f9927d6a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4027f-2bc5-45d6-87a5-970646d17cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f21ba-3edf-4dc3-9a3f-f3c8ca94ca2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3034e5-9d8f-4af2-bf9d-0633beb3e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f488-156a-4d7b-ae34-f0a00aadc11b",
   "metadata": {},
   "source": [
    "## Get previous Pandas dataframe and put it into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8461e3f-e59e-49fd-ada9-67c88443003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Done\n",
    "#\n",
    "#from forex.pre_training_data_prep.tasks.spark.task_convert_pandas_df_to_spark_df import task_convert_pandas_df_to_spark_df\n",
    "#task_convert_pandas_df_to_spark_df(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a29b58-d7ea-479d-b384-ddf9ce587223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_conversion_to_spark'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919521e9-e627-4455-a687-1ddfd93b28e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad54d63-fc5b-4dfc-9d7a-609eb70d3e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0fc35-1b7b-4a0e-986e-a19c6da0d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Done\n",
    "#\n",
    "#from forex.pre_training_data_prep.tasks.spark.array_tasks import task_pivot_and_sort_arrays\n",
    "#task_pivot_and_sort_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532645b-85ef-421e-8ee1-a7f30b83a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Done\n",
    "#\n",
    "#from forex.pre_training_data_prep.tasks.spark.array_tasks import task_diff_the_timestamp_arrays\n",
    "#task_diff_the_timestamp_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de77202-1113-416c-bdce-d75479db1865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041391e9-06d3-4d18-9ead-426ae1322623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_pivot_and_sort'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5091c-7f32-461d-be98-70672b2296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_timestamp_diff'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f010901-6f60-45fa-855f-1eb17e96acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c755d6-4493-46ed-bbc5-137ebc838bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b111045-53b0-49b1-a0f1-4cc1fb92c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5d6e-c904-43e6-ad48-bfc5a70ecc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_timestamps(timestamp_array, seconds_divisor):\n",
    "    return [int(x) for x in range(min(timestamp_array), max(timestamp_array) + seconds_divisor, seconds_divisor)]\n",
    "\n",
    "udf_get_all_timestamps = f.udf(get_all_timestamps, ArrayType(IntegerType()))\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def do_nans_exist(values_array):\n",
    "#    values_array = np.array([np.array(values_array)])\n",
    "#    mask = np.isnan(values_array)\n",
    "#    has_nan_0_or_1 = np.max([int(x) for x in mask[0]])\n",
    "#    return int(has_nan_0_or_1)\n",
    "#\n",
    "#udf_do_nans_exist = f.udf(do_nans_exist, IntegerType())\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def do_non_nans_exist(values_array):\n",
    "#    values_array = np.array([np.array(values_array)])\n",
    "#    mask = ~np.isnan(values_array)\n",
    "#    has_non_nan_0_or_1 = np.max([int(x) for x in mask[0]])\n",
    "#    return int(has_non_nan_0_or_1)\n",
    "#    \n",
    "#udf_do_non_nans_exist = f.udf(do_non_nans_exist, IntegerType())\n",
    "\n",
    "\n",
    "def count_nans_in_array(values_array):\n",
    "    values_array = np.array([np.array(values_array)])\n",
    "    mask = np.isnan(values_array)\n",
    "    nan_count = np.sum([int(x) for x in mask[0]])\n",
    "    return int(nan_count)\n",
    "    \n",
    "udf_count_nans_in_array = f.udf(count_nans_in_array, IntegerType())\n",
    "\n",
    "\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def forward_fill(values_array):\n",
    "#    arr = np.array([values_array])\n",
    "#    mask = np.isnan(arr)\n",
    "#    idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "#    np.maximum.accumulate(idx, axis = 1, out = idx)\n",
    "#    arr[mask] = arr[np.nonzero(mask)[0], idx[mask]]\n",
    "#    to_return = list([float(x) for x in arr[0]])\n",
    "#    return to_return\n",
    "#\n",
    "#udf_forward_fill = f.udf(forward_fill, ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "def locate_nans(timestamp_array, timestamp_all_array, values_array):\n",
    "\n",
    "    # make sure we get an argsort in here later to ensure order of values is correct\n",
    "\n",
    "    ts = np.array(timestamp_array, dtype = np.uint64) # ??\n",
    "    ts_all = np.array(timestamp_all_array, dtype = np.uint64)  # we can probably make this smaller\n",
    "    v = np.array(values_array, dtype = np.float64)  # we can probably make this smaller\n",
    "    \n",
    "    pdf = pd.DataFrame({'timestamp' : ts, 'values' : v})\n",
    "    pdf_all = pd.DataFrame({'timestamp' : ts_all})\n",
    "\n",
    "    pdf_joined = (\n",
    "        pd.merge(\n",
    "            pdf_all,\n",
    "            pdf,\n",
    "            on = 'timestamp',\n",
    "            how = 'left',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_return = pdf_joined['values'].to_list()\n",
    "    \n",
    "    return to_return\n",
    "\n",
    "udf_locate_nans = f.udf(locate_nans, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140de41-d08c-43a6-bda2-c9449c586e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_get_all_timestamps\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_locate_nans\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_count_nans_in_array\n",
    "\n",
    "def task_find_full_day_nans(**config):\n",
    "\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = (\n",
    "        spark.read.parquet(config['directory_output'] + '/' + config['filename_timestamp_diff'])\n",
    "        .coalesce(config['n_processors_to_coalesce'])\n",
    "        .orderBy('date_post_shift')\n",
    "        .withColumn(\n",
    "            'timestamps_all',\n",
    "            udf_get_all_timestamps(f.col('sorted_timestamp_array'), f.lit(config['seconds_divisor']))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .withColumn(\n",
    "                item + '_and_nans',\n",
    "                udf_locate_nans(f.col('sorted_timestamp_array'), f.col('timestamps_all'), f.col('sorted_' + item + '_array'))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .withColumn(\n",
    "                item + '_nan_count',\n",
    "                udf_count_nans_in_array(f.col(item + '_and_nans'))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sdf_arrays = sdf_arrays.withColumn('nan_count_full_day', f.col('return_nan_count'))\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = sdf_arrays.drop(item + '_nan_count')\n",
    "\n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1dc0fa-b1c1-4205-b576-9f940443a478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ca603-60fb-49bc-b082-9a9053483b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ef26a-af40-4c49-81a1-7ae40ff0c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import task_find_full_day_nans\n",
    "task_find_full_day_nans(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbba26-f489-4c11-9838-74786804c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead3234-e02e-474d-85fa-4f2bce5efdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d577b-a702-42d0-a599-6602e3368b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f3743-c749-4abc-8775-4da781b07f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99835ce-436d-4ce5-b037-dda94247ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0e2ab-7762-4425-970a-69c54c1372d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869a62e-b9be-47dd-af65-0ff624032788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QA #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a5fa6-e74e-48d4-a2d3-b88129e6ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['directory_output'] = config['directory_output'] + '/manual__2025-03-19T08:37:57.595997+00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379dee1-8b87-4361-89f8-e1a1a90edabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import numpy as np\n",
    "from utilities.spark_session import get_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24ce60-af98-4caa-91f4-0ef296219213",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_qa = (\n",
    "    spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    .select('nan_count_full_day', 'date_post_shift')\n",
    "    .orderBy(f.col('nan_count_full_day').desc())\n",
    "    .withColumn('day_has_nans', f.col('nan_count_full_day') > 0)\n",
    "    .groupBy('day_has_nans').agg(f.count('date_post_shift').alias('number_of_days'))\n",
    ")\n",
    "sdf_qa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa66a-0472-4613-90d9-a9fc2c60344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_qa_counts = (\n",
    "    spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    .select('nan_count_full_day', 'date_post_shift')\n",
    "    .orderBy(f.col('nan_count_full_day').desc())\n",
    ")\n",
    "sdf_qa_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4746b9-3cb9-4765-8eab-f5d521c77e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nan_counts_per_day = [x['nan_count_full_day'] for x in sdf_qa_counts.select('nan_count_full_day').collect()]\n",
    "\n",
    "#plt.figure()\n",
    "#plt.hist(nan_counts_per_day)\n",
    "#plt.show()\n",
    "#plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot([nan_counts_per_day], widths=0.95)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ed94-8001-4b0a-bdfe-888a7c2d0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(nan_counts_per_day), np.median(nan_counts_per_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe322e8-0724-4d34-b6c7-29647d9c10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_qa_dates = sdf_qa_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97eb5c-455e-4996-931d-668cacb9df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_qa_dates['weekday'] = [x.weekday() for x in pdf_qa_dates['date_post_shift']]\n",
    "\n",
    "pdf_qa_dates.boxplot(by = 'weekday')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a7d5-147e-4b43-839f-c393d83874fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## end QA #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f97ad-492d-4398-a441-6ec94ce07380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf951bc-53bf-497a-81a7-03e8a6c2c4e6",
   "metadata": {},
   "source": [
    "## Investigate consecutive NaNs for each day\n",
    "\n",
    "By identifying the maximum number of NaNs in a row for a given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94103cc2-9e75-4926-8d29-c5c9d3f4759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "run_id = 'manual__2025-03-19T08:37:57.595997+00:00'  # TEMP\n",
    "\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_full_day_nans'])\n",
    "\n",
    "stuff_1 = sdf_arrays.columns\n",
    "\n",
    "sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86d06b-8dbb-407a-b2ce-1bfb3e2b0417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347f9e9-05dc-4339-a30b-510b7986f038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#from itertools import groupby\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "def booger(**config):\n",
    "\n",
    "    from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_get_max_consecutive_NaNs\n",
    "    from utilities.test_all_equality import udf_test_all_equality\n",
    "    \n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    run_id = config['dag_run'].run_id\n",
    "    sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_full_day_nans'])\n",
    "\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = sdf_arrays.withColumn('max_consec_nan_' + item, udf_get_max_consecutive_NaNs(f.col(item + '_and_nans')))\n",
    "\n",
    "    columns_to_test = [f.col('max_consec_nan_' + x) for x in config['list_data_columns']]\n",
    "    sdf_arrays = sdf_arrays.withColumn('is_consec_equal', udf_test_all_equality(*columns_to_test))\n",
    "\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn('max_daily_consec_nans', f.col('max_consec_nan_' + config['list_data_columns'][0]))\n",
    "    )\n",
    "\n",
    "    # clean up\n",
    "    sdf_arrays = sdf_arrays.drop('timestamps_all')\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .drop(\n",
    "                #'max_consec_nan_' + item,\n",
    "                item + '_and_nans',\n",
    "            )\n",
    "        )\n",
    "    spark.stop()                                       \n",
    "\n",
    "\n",
    "booger(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#columns_to_select = ['date_post_shift', 'nan_count_full_day']\n",
    "#columns_to_select.extend(['max_consec_nan_' + x for x in config['list_data_columns']])\n",
    "#columns_to_select.append('is_consec_equal')\n",
    "#\n",
    "#sdf_temp = sdf_arrays.select(*columns_to_select)\n",
    "#\n",
    "##sdf_temp.show(5)\n",
    "#\n",
    "## should all be true\n",
    "#sdf_agg = (\n",
    "#    sdf_temp.select('date_post_shift', 'is_consec_equal')\n",
    "#    .groupBy('is_consec_equal')\n",
    "#    .agg(f.count('date_post_shift').alias('count_column'))\n",
    "#)\n",
    "##sdf_agg.show()\n",
    "#\n",
    "#\n",
    "##sdf_arrays = (\n",
    "##    sdf_arrays\n",
    "##    .withColumn('max_daily_consec_nans', f.col('max_consec_nan_' + config['list_data_columns'][0]))\n",
    "##    .drop('is_consec_equal')\n",
    "##    .drop('sorted_timestamp_array', 'diff_sorted_timestamp_array')\n",
    "##)\n",
    "#\n",
    "##for item in config['list_data_columns']:\n",
    "##    sdf_arrays = (\n",
    "##        sdf_arrays\n",
    "##        .drop(\n",
    "##            'max_consec_nan_' + item,\n",
    "##            'sorted_' + item + '_array',\n",
    "##        )\n",
    "##    )\n",
    "#\n",
    "#\n",
    "##\n",
    "## length tests\n",
    "##\n",
    "#sdf_arrays = sdf_arrays.withColumn('len_timestamps_all', f.array_size(f.col('timestamps_all')))\n",
    "#for item in config['list_data_columns']:\n",
    "#    sdf_arrays = sdf_arrays.withColumn('len_' + item, f.array_size(f.col(item + '_and_nans')))\n",
    "#\n",
    "#columns_to_test = [f.col('len_' + x) for x in config['list_data_columns']]\n",
    "#columns_to_test.extend(['len_timestamps_all'])\n",
    "#\n",
    "#sdf_arrays = sdf_arrays.withColumn('is_length_equal', udf_test_all_equality(*columns_to_test))\n",
    "#\n",
    "## True\n",
    "#sdf_agg = (\n",
    "#    sdf_arrays.select('date_post_shift', 'is_length_equal')\n",
    "#    .groupBy('is_length_equal')\n",
    "#    .agg(f.count('date_post_shift').alias('count_column'))\n",
    "#)\n",
    "##sdf_agg.show()\n",
    "#\n",
    "#sdf_arrays = sdf_arrays.drop('is_length_equal', 'len_timestamps_all')\n",
    "#for item in config['list_data_columns']:\n",
    "#    sdf_arrays = sdf_arrays.drop('len_' + item)\n",
    "#\n",
    "#sdf_arrays.show(3)\n",
    "#\n",
    "#sdf_arrays.drop('nan_count_full_day', 'max_daily_consec_nans').write.mode('overwrite').parquet(config['directory_output'] + '/' + 'PLACEHOLDER.parquet')\n",
    "#\n",
    "\n",
    "#sdf_arrays = sdf_arrays.drop('nan_count_full_day', 'max_daily_consec_nans')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac503c2b-046c-468b-8b44-18751f19dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "run_id = 'manual__2025-03-21T03:30:18.253565+00:00'  # temp\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_full_day_nans'])\n",
    "print(sdf_arrays.columns)\n",
    "print()\n",
    "sdf_arrays.show(3)\n",
    "print()\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/QA/' + config['filename_qa_day_nan_counts'])\n",
    "print(sdf_arrays.columns)\n",
    "print()\n",
    "sdf_arrays.show(3)\n",
    "print()\n",
    "\n",
    "#print()\n",
    "#print(config['filename_qa_full_day_consecutive_nans'])\n",
    "#print()\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/QA/' + config['filename_qa_full_day_consecutive_nans'])\n",
    "print(sdf_arrays.columns)\n",
    "print()\n",
    "sdf_arrays.show(3)\n",
    "print()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bccfeb-bda4-46ca-80ad-79158522a855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054af377-9120-4645-8406-58103d0a7c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56981529-efbd-4f91-9a14-2c84aa25caba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc800ee2-367e-41b0-8807-a7c91348049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_2 = sdf_arrays.columns\n",
    "print(stuff_1)\n",
    "print()\n",
    "print(stuff_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dd0c6-fa75-4496-b083-c2876215aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071eba9d-b932-416c-8fc2-2f54781170cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b2e71-a864-420b-b186-60f35b3f5aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0faab4-4a04-492e-b0a6-65fc6b16231a",
   "metadata": {},
   "source": [
    "## Trig functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b2769-aa75-4d4b-91c8-62f556823133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.trig import add_trig\n",
    "add_trig(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2595f74-5b58-4941-953f-084ce7a23d57",
   "metadata": {},
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564da7b-70e5-4b0d-812e-05f204f31e89",
   "metadata": {},
   "source": [
    "#### Ensure there is room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaeda6d-6b4e-4393-a1e4-945e1ba5ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.spark_sliding_window import test_window_space\n",
    "test_window_space(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d20295-73b7-4929-9544-645ba4ab5e4b",
   "metadata": {},
   "source": [
    "#### Compute the sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3861d81-dcec-4a53-93c7-6973bae33227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.spark_sliding_window import do_sliding_window\n",
    "do_sliding_window(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cac234c-28d1-4593-9129-2c7837a8249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|date_post_shift|      timestamps_all|     return_and_nans| volatility_and_nans|     volume_and_nans|   lhc_mean_and_nans|              sin_24|              cos_24|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     2021-05-14|[1620939660, 1620...|[0.0, -3.9E-4, 1....|[0.0, 4.6E-4, 1.1...|[1.0, 6.0, 2.0, N...|[1.20806, 1.20789...|[-0.7040147, -0.7...|[0.71018535, 0.71...|\n",
      "|     2021-05-17|[1621198800, 1621...|[-2.4E-4, -5.9E-4...|[2.4E-4, 5.9E-4, ...|[4.0, 4.0, 4.0, N...|[1.21458, 1.21419...|[-0.70710677, -0....|[0.70710677, 0.71...|\n",
      "|     2021-05-18|[1621285260, 1621...|[0.0, -6.0E-5, Na...|[0.0, 6.0E-5, NaN...|[1.0, 3.0, NaN, N...|[1.21528, 1.21524...|[-0.7040147, -0.7...|[0.71018535, 0.71...|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "+--------------+------------+\n",
      "|is_long_enough|count_column|\n",
      "+--------------+------------+\n",
      "|          true|        4246|\n",
      "|         false|           1|\n",
      "+--------------+------------+\n",
      "\n",
      "\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|date_post_shift|        sw_timestamp|           sw_return|       sw_volatility|           sw_volume|         sw_lhc_mean|           sw_sin_24|           sw_cos_24|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     2016-04-13|[[1460494860, 146...|[[4.0E-5, -1.2E-4...|[[4.0E-5, 2.0E-4,...|[[2.0, 13.0, 3.0,...|[[1.1384866, 1.13...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "|     2016-04-14|[[1460581260, 146...|[[3.0E-5, NaN, Na...|[[3.0E-5, NaN, Na...|[[2.0, NaN, NaN, ...|[[1.12739, NaN, N...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "|     2016-04-15|[[1460667660, 146...|[[0.0, 2.0E-5, -5...|[[2.0E-5, 6.0E-5,...|[[4.0, 4.0, 8.0, ...|[[1.12635, 1.1263...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "run_id = 'manual__2025-03-21T04:16:18.017637+00:00'\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/spark_sliding_window_space_check.parquet')\n",
    "sdf_arrays.show(3)\n",
    "print()\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/QA/spark_sliding_window_QA.parquet')\n",
    "sdf_arrays.show(3)\n",
    "print()\n",
    "\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_sliding_window'])\n",
    "sdf_arrays.show(3)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e158ee6-a8a3-4c21-9340-b4f891f70df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20032eb9-5945-4f18-a5e5-6b8f26250f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077611fd-8921-4c10-bb8d-82ede415051b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04f58e-5e1a-4b29-ac93-304f66ed237c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9783d-c191-4f96-a5ec-1b2d706aa522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb181b3-b7e4-4be0-a722-105cbeca147d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1840a46-8729-4d8d-a7af-187977c7ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from forex.pre_training_data_prep.config import config\n",
    "#from utilities.spark_session import get_spark_session\n",
    "#spark = get_spark_session(config['spark_config'])\n",
    "#sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_sliding_window_space_check'])\n",
    "#sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad4717-a4ce-4b81-bbc8-cc41c7672d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71266437-f20a-46b9-a6ee-78411bc873ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef24efb-61e0-49d9-b946-b1762afc50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb5a379-7054-4062-aebc-60a3f385ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|date_post_shift|        sw_timestamp|           sw_return|       sw_volatility|           sw_volume|         sw_lhc_mean|           sw_sin_24|           sw_cos_24|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     2016-04-13|[[1460494860, 146...|[[4.0E-5, -1.2E-4...|[[4.0E-5, 2.0E-4,...|[[2.0, 13.0, 3.0,...|[[1.1384866, 1.13...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "|     2016-04-14|[[1460581260, 146...|[[3.0E-5, NaN, Na...|[[3.0E-5, NaN, Na...|[[2.0, NaN, NaN, ...|[[1.12739, NaN, N...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "|     2016-04-15|[[1460667660, 146...|[[0.0, 2.0E-5, -5...|[[2.0E-5, 6.0E-5,...|[[4.0, 4.0, 8.0, ...|[[1.12635, 1.1263...|[[-0.7040147, -0....|[[0.71018535, 0.7...|\n",
      "+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "run_id = 'manual__2025-03-21T04:16:18.017637+00:00'\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_sliding_window'])\n",
    "sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9526cb71-efaa-4e57-80d7-66f933a79343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "def testI(a_list_of_lists):\n",
    "    i = a_list_of_lists[0][0]\n",
    "    return i\n",
    "\n",
    "def testJ(a_list_of_lists):\n",
    "    j = a_list_of_lists[20][0]\n",
    "    return j\n",
    "\n",
    "udf_test_int_i = f.udf(testI, IntegerType())\n",
    "udf_test_int_j = f.udf(testJ, IntegerType())\n",
    "\n",
    "udf_test_float_i = f.udf(testI, FloatType())\n",
    "udf_test_float_j = f.udf(testJ, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b464faf8-4619-44ff-906c-d5f5566e429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+\n",
      "|date_post_shift|         I|         J|\n",
      "+---------------+----------+----------+\n",
      "|     2016-04-13|1460494860|1460506860|\n",
      "|     2016-04-14|1460581260|1460593260|\n",
      "|     2016-04-15|1460667660|1460679660|\n",
      "+---------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sdf_arrays\n",
    "    .withColumn('I', udf_test_int_i(f.col('sw_timestamp')))\n",
    "    .withColumn('J', udf_test_int_j(f.col('sw_timestamp')))\n",
    "    .select('date_post_shift', 'I', 'J')\n",
    "    .show(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd7530ef-be92-4690-bc5e-e68afefc482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+-------+\n",
      "|date_post_shift|     I|      J|\n",
      "+---------------+------+-------+\n",
      "|     2016-04-13|4.0E-5| 6.0E-5|\n",
      "|     2016-04-14|3.0E-5|-6.0E-5|\n",
      "|     2016-04-15|   0.0|-6.0E-5|\n",
      "+---------------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sdf_arrays\n",
    "    .withColumn('I', udf_test_float_i(f.col('sw_return')))\n",
    "    .withColumn('J', udf_test_float_j(f.col('sw_return')))\n",
    "    .select('date_post_shift', 'I', 'J')\n",
    "    .show(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33baf63a-3e59-4ec1-9ba4-72e96591e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+\n",
      "|date_post_shift|         I|          J|\n",
      "+---------------+----------+-----------+\n",
      "|     2016-04-13|-0.7040147|0.091501616|\n",
      "|     2016-04-14|-0.7040147|0.091501616|\n",
      "|     2016-04-15|-0.7040147|0.091501616|\n",
      "+---------------+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sdf_arrays\n",
    "    .withColumn('I', udf_test_float_i(f.col('sw_sin_24')))\n",
    "    .withColumn('J', udf_test_float_j(f.col('sw_sin_24')))\n",
    "    .select('date_post_shift', 'I', 'J')\n",
    "    .show(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ec6372-5076-4caa-82fd-46a87cc0dcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+-----------+-----------+-----------+-----------+\n",
      "|date_post_shift|I_sw_timestamp|J_sw_timestamp|I_sw_return|J_sw_return|I_sw_sin_24|J_sw_sin_24|\n",
      "+---------------+--------------+--------------+-----------+-----------+-----------+-----------+\n",
      "|     2016-04-13|    1460494860|    1460506860|     4.0E-5|     6.0E-5| -0.7040147|0.091501616|\n",
      "|     2016-04-14|    1460581260|    1460593260|     3.0E-5|    -6.0E-5| -0.7040147|0.091501616|\n",
      "|     2016-04-15|    1460667660|    1460679660|        0.0|    -6.0E-5| -0.7040147|0.091501616|\n",
      "+---------------+--------------+--------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sdf_arrays\n",
    "    .withColumn('I_sw_timestamp', udf_test_int_i(f.col('sw_timestamp')))\n",
    "    .withColumn('J_sw_timestamp', udf_test_int_j(f.col('sw_timestamp')))\n",
    "    .withColumn('I_sw_return', udf_test_float_i(f.col('sw_return')))\n",
    "    .withColumn('J_sw_return', udf_test_float_j(f.col('sw_return')))\n",
    "    .withColumn('I_sw_sin_24', udf_test_float_i(f.col('sw_sin_24')))\n",
    "    .withColumn('J_sw_sin_24', udf_test_float_j(f.col('sw_sin_24')))\n",
    "    .select(\n",
    "        'date_post_shift',\n",
    "        'I_sw_timestamp', 'J_sw_timestamp',\n",
    "        'I_sw_return', 'J_sw_return',\n",
    "        'I_sw_sin_24', 'J_sw_sin_24',\n",
    "    )\n",
    "    .show(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205fa643-58f6-4581-b28c-af1ff341e7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6f47c3-fac9-46b1-b4d8-1137e693db04",
   "metadata": {},
   "source": [
    "## Expand arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d7e0d-6e83-4c3b-87e6-1969455734ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from forex.pre_training_data_prep.config import config\n",
    "#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff12d52-e201-455f-ad4f-853eaba6f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from forex.pre_training_data_prep.tasks.spark.array_expand import task_expand_arrays\n",
    "#task_expand_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24a1772d-2ffb-4ec5-8b10-f6975809ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|date_post_shift| timestamp|              return|          volatility|              volume|            lhc_mean|              sin_24|              cos_24|\n",
      "+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     2014-12-15|1418604600|[6.0E-5, -2.0E-4,...|[1.8E-4, 2.2E-4, ...|[21.0, 20.0, 34.0...|[1.2457334, 1.245...|[0.21643962, 0.22...|[0.976296, 0.9753...|\n",
      "|     2014-12-15|1418605200|[5.0E-5, -2.0E-5,...|[1.3E-4, 1.3E-4, ...|[11.0, 22.0, 18.0...|[1.24569, 1.24575...|[0.25881904, 0.26...|[0.9659258, 0.964...|\n",
      "|     2014-12-15|1418605800|[-1.2E-4, -4.0E-5...|[1.7E-4, 4.0E-5, ...|[14.0, 8.0, 12.0,...|[1.2454033, 1.245...|[0.3007058, 0.304...|[0.95371693, 0.95...|\n",
      "+---------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "run_id = 'manual__2025-03-22T03:30:14.026683+00:00'\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + run_id + '/' + config['filename_explode_array'])\n",
    "sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5def26ca-01ae-4b59-b2fc-257f1acf8aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523511"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_arrays.count()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcd728-a349-4ea2-a6a8-b3de37ea0c97",
   "metadata": {},
   "source": [
    "## Find NaNs and decide what to do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d79b74-7726-4486-940d-69d017b7670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1a749-e7f2-47f5-ba33-fc7b4aebc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_count_nans_in_array\n",
    "#from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_count_nans_in_array\n",
    "# need something about first list item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a085ee-d9eb-455c-a248-a905bb47874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_get_max_consecutive_NaNs\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "def deal_with_post_sw_nans(**config):\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_explode_array'])\n",
    "\n",
    "    items_to_check_list = config['list_data_columns'].copy()\n",
    "    items_to_check_list.extend(config['list_data_columns_no_scale'])\n",
    "    \n",
    "    for item in items_to_check_list:\n",
    "        sdf_arrays = sdf_arrays.withColumn('max_consec_nan_' + item, udf_get_max_consecutive_NaNs(f.col(item)))\n",
    "\n",
    "    # We are skipping the QA equality test(s) here. Add it later.\n",
    "\n",
    "    # is first item NaN?\n",
    "    # drop if yes\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn('is_first_item_a_nan', f.isnan(f.col('return').getItem(0)))\n",
    "        .where(~f.col('is_first_item_a_nan'))\n",
    "        .drop('is_first_item_a_nan')\n",
    "    )\n",
    "\n",
    "    # max consecutive NaN count\n",
    "    # decide on a threshold value after plotting the distribution\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn('max_consec_nans', f.col('max_consec_nan_return'))\n",
    "        .drop(*['max_consec_nan_' + x for x in items_to_check_list])\n",
    "    )\n",
    "\n",
    "    # Again, we are skipping the QA equality test(s) here. Add it later.\n",
    "    for item in items_to_check_list:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .withColumn(\n",
    "                item + '_total_nan_count',\n",
    "                udf_count_nans_in_array(f.col(item))\n",
    "            )\n",
    "        )\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn('total_nan_count', f.col(config['list_data_columns'][0] + '_total_nan_count'))\n",
    "        .drop(*[x + '_total_nan_count' for x in items_to_check_list])\n",
    "    )\n",
    "\n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_post_sw_nans'])    \n",
    "    spark.stop()\n",
    "\n",
    "deal_with_post_sw_nans(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa7fae-3109-4167-8c36-b9a2a8d2486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_with_post_sw_nans(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac598280-7236-439b-9447-f1f3b8830589",
   "metadata": {},
   "source": [
    "## Filter by NaN counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144d9d2-dd85-4f7b-9f8e-42401c13188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e773192-1ba5-4a21-9781-283fca9030a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import plot_post_sw_nans\n",
    "plot_post_sw_nans(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e44ac6-6e33-4fb0-8f2e-996de5579d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import filter_by_nan_counts\n",
    "filter_by_nan_counts(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7d75f-b57f-414a-8e36-228805988ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48751ec-541e-4fb5-9a7c-9fda136c7cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe9eec-fee4-4c2b-a630-a9fc01f9f714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c560fbad-546e-4aee-b99a-ed3fcaeed201",
   "metadata": {},
   "source": [
    "## Compute mean and standard deviation\n",
    "\n",
    "(Before forward-fill; after implementing the forward-fill operation we'll perform the actual scaling procedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeeabc-f77d-478a-9589-6cb6ebdc306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c2885-a928-4d81-a164-057f57d02850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.scale_it_all import compute_scaling_statistics_for_later_use\n",
    "compute_scaling_statistics_for_later_use(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eac322-6349-48f3-b320-d60595261179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_scaling_stats'])\n",
    "sdf_arrays.show(3)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03817b-9574-4191-8369-f059e017c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf8451-4b49-4698-bfcd-ec6cdd2a6ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af264e8a-9981-4f49-b2c2-bbcbf267399a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18fecd62-c314-4c52-8716-ce561d1e52d0",
   "metadata": {},
   "source": [
    "## Forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb2f67-30bc-4643-99c5-ce39bd4a8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b71a72-50a8-451e-9cd9-447b84c761e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import forward_fill_it_all\n",
    "forward_fill_it_all(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088b525-3c3d-4392-9cb9-c7bb8df634b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_forward_filled'])\n",
    "sdf_arrays.show(3)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cd7e3-e3ec-4d90-a744-d4329493dcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25909bac-5f5b-41ca-a980-1dc5cbfc7a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f13adfa-1c16-4b6a-be8b-325cb1f90c2b",
   "metadata": {},
   "source": [
    "## Properly scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a9168-7ecc-48b3-89f1-764ef32c721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104d727-b853-4251-b010-49a79e24fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.scale_it_all import properly_scale_it_all\n",
    "properly_scale_it_all(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baff6a3-dc83-4b79-88b2-e540a8223f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_scaled'])\n",
    "sdf_arrays.show(3)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adb504-72b3-4890-9928-bedd993ed0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252848ba-4820-44d2-8c58-bd28e09e4ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43854763-c7bb-49ca-837b-b7a06e66e57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f20949-9f54-4ab8-901b-acef47b48b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c327b29-2f71-4e62-9661-80b3b2b6f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "def stuff(**config):\n",
    "    np.random.seed(config['shuffle_random_seed'])\n",
    "    \n",
    "    dict_results = {}\n",
    "    \n",
    "    pdf = pd.read_parquet(config['directory_output'] + '/' + config['filename_scaled'])\n",
    "    pdf.columns = [x.replace('_scaled', '').replace('_ff', '') for x in pdf.columns]\n",
    "    \n",
    "    item_list = []\n",
    "    item_list.extend(config['list_data_columns'])\n",
    "    item_list.extend(config['list_data_columns_no_scale'])\n",
    "\n",
    "    dict_results['features'] = {\n",
    "        'features_list' : item_list,\n",
    "        'features_index_lookup' : {\n",
    "            'name_to_index' : {},\n",
    "            'index_to_name' : {},\n",
    "        },\n",
    "    }\n",
    "    for i, item in enumerate(item_list):\n",
    "        dict_results['features']['features_index_lookup']['index_to_name'][i] = item\n",
    "        dict_results['features']['features_index_lookup']['name_to_index'][item] = i\n",
    "\n",
    "    # ensure we have the columns completely named how we want them\n",
    "    new_column_names = ['date_post_shift', 'timestamp']\n",
    "    new_column_names.extend(item_list)\n",
    "    new_column_names.extend(['mean_' + item for item in item_list if item in config['list_data_columns']])\n",
    "    new_column_names.extend(['std_' + item for item in item_list if item in config['list_data_columns']])\n",
    "\n",
    "    pdf = pdf[new_column_names]\n",
    "    pdf.sort_values(by = ['timestamp'], inplace = True)  # I probably did this earlier but don't want to fuss with the matter right now\n",
    "\n",
    "    dict_results['list_of_timestamps'] = [int(x) for x in pdf['timestamp'].values]\n",
    "    dict_results['list_of_shifted_dates'] = [str(x) for x in pdf['date_post_shift'].values]\n",
    "\n",
    "    n_back = config['n_back']\n",
    "    n_forward = config['n_forward']\n",
    "\n",
    "    M_array = pdf[item_list].to_numpy()\n",
    "    n_rows, n_features = M_array.shape\n",
    "    \n",
    "    # I'm sure there is a more elegant way to do this--without an explicitly programmed loop, that is\n",
    "    #\n",
    "    M_Xy = np.empty([n_rows, n_back + n_forward, n_features])\n",
    "    for row in range(0, n_rows):\n",
    "        for feature_track in range(0, n_features):\n",
    "            M_Xy[row, :, feature_track] = M_array[row, feature_track]\n",
    "\n",
    "    # HARD CODED:  Should be optional\n",
    "    if True:\n",
    "        n_step_to_use_here = 5  # should be in config\n",
    "        M_Xy = M_Xy[::n_step_to_use_here, :, :]   # see if we use n_step any where else, as its use here might not be correct\n",
    "        \n",
    "    # this assumes we want chronological order\n",
    "    indices_train = np.uint64(np.arange(0, np.uint64(np.floor(np.round(M_Xy.shape[0] * config['train_val_test_split'][0])))))\n",
    "    indices_val = np.uint64(np.arange(indices_train[-1] + 1, np.uint64(np.floor(np.round(indices_train[-1] + M_Xy.shape[0] * config['train_val_test_split'][1])))))\n",
    "    indices_test = np.uint64(np.arange(indices_val[-1] + 1, np.uint64(np.floor(np.round(indices_val[-1] + M_Xy.shape[0] * config['train_val_test_split'][2])))))\n",
    "\n",
    "    if config['shuffle_X_train_and_val']:\n",
    "        np.random.shuffle(indices_train)\n",
    "        np.random.shuffle(indices_val)\n",
    "        np.random.shuffle(indices_test)\n",
    "\n",
    "    M_Xy_train = M_Xy[indices_train, :, :]\n",
    "    M_Xy_val = M_Xy[indices_val, :, :]\n",
    "    M_Xy_test = M_Xy[indices_test, :, :]\n",
    "\n",
    "    #\n",
    "    # the following could have been condensed into one for loop, but I decided for ease of debugging to code this operation very specifically\n",
    "    #\n",
    "    dict_results['matrices'] = {'train' : {}, 'val' : {}, 'test' : {}}\n",
    "    dict_results['matrices']['train'] = {\n",
    "        'X' : M_Xy_train[:, 0:n_back, :],\n",
    "        'y' : M_Xy_train[:, n_back:(n_back + n_forward), :],\n",
    "    }\n",
    "    dict_results['matrices']['val'] = {\n",
    "        'X' : M_Xy_val[:, 0:n_back, :],\n",
    "        'y' : M_Xy_val[:, n_back:(n_back + n_forward), :],\n",
    "    }\n",
    "    dict_results['matrices']['test'] = {\n",
    "        'X' : M_Xy_test[:, 0:n_back, :],\n",
    "        'y' : M_Xy_test[:, n_back:(n_back + n_forward), :],\n",
    "    }\n",
    "    \n",
    "    with open(config['directory_output'] + '/' + config['filename_numpy_final_dict'], 'wb') as fff:\n",
    "        pickle.dump(dict_results, fff)\n",
    "    \n",
    "    # save the reorganized pandas dataframe\n",
    "    pdf.to_parquet(config['directory_output'] + '/' + config['filename_final_pandas_df'])\n",
    "\n",
    "\n",
    "stuff(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d9931-7fe8-4977-9911-be28526aabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(config['directory_output'] + '/' + config['filename_numpy_final_dict'], 'rb') as fff:\n",
    "#    dict_results = pickle.load(fff)\n",
    "\n",
    "#print(dict_results["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4aab4-187f-47ef-a2bb-06e5471da658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah----9101c2e0-43cb-4a70-9aff-8680f73edf62  -- first run with this code, all of y predicted\n",
    "# blah----de18c0e6-8907-4c4b-ac9a-1b1c96e4b477  -- second run with this code, mean of y predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
