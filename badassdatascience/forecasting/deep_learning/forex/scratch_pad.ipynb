{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a696865-0400-4a18-a8dc-8f9927d6a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4027f-2bc5-45d6-87a5-970646d17cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3034e5-9d8f-4af2-bf9d-0633beb3e782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database_name': 'django',\n",
       " 'dag_id': 'NEW_prepare_forex_data',\n",
       " 'list_data_columns': ['return', 'volatility', 'volume', 'lhc_mean'],\n",
       " 'list_data_columns_no_scale': ['sin_24', 'cos_24'],\n",
       " 'n_back': 180,\n",
       " 'n_forward': 30,\n",
       " 'offset': 1,\n",
       " 'n_step': 10,\n",
       " 'tz_name': 'US/Eastern',\n",
       " 'price_type_name': 'mid',\n",
       " 'instrument_name': 'EUR/USD',\n",
       " 'interval_name': 'Minute',\n",
       " 'retries_pull_forex_data': 1,\n",
       " 'retry_delay_minutes_pull_forex_data': 5,\n",
       " 'directory_output': '/home/emily/Desktop/projects/test/badass-data-science/badassdatascience/forecasting/deep_learning/forex/output',\n",
       " 'filename_candlesticks_query_results': 'candlesticks_query_results.parquet',\n",
       " 'filename_timezone_added': 'candlesticks_timezone_added.parquet',\n",
       " 'filename_offset': 'candlesticks_timezone_weekday_offset.parquet',\n",
       " 'filename_weekday_shift_merged': 'candlesticks_weekday_offset_merged.parquet',\n",
       " 'filename_shift_days_and_hours_as_needed': 'candlesticks_shifted_as_needed.parquet',\n",
       " 'filename_finalized_pandas': 'candlesticks_finalized_pandas.parquet',\n",
       " 'filename_conversion_to_spark': 'spark_converted.parquet',\n",
       " 'filename_pivot_and_sort': 'spark_pivot_and_sort.parquet',\n",
       " 'filename_timestamp_diff': 'spark_timestamp_diff.parquet',\n",
       " 'filename_full_day_nans': 'spark_full_day_nans.parquet',\n",
       " 'filename_post_trig': 'spark_post_trig.parquet',\n",
       " 'spark_config': [('spark.executor.memory', '100g'),\n",
       "  ('spark.executor.cores', '20'),\n",
       "  ('spark.cores.max', '20'),\n",
       "  ('spark.driver.memory', '100g'),\n",
       "  ('spark.sql.execution.arrow.pyspark.enabled', 'true')],\n",
       " 'n_processors_to_coalesce': 20,\n",
       " 'seconds_divisor': 60}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from forex.pre_training_data_prep.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f488-156a-4d7b-ae34-f0a00aadc11b",
   "metadata": {},
   "source": [
    "## Get previous Pandas dataframe and put it into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8461e3f-e59e-49fd-ada9-67c88443003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.task_convert_pandas_df_to_spark_df import task_convert_pandas_df_to_spark_df\n",
    "task_convert_pandas_df_to_spark_df(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a29b58-d7ea-479d-b384-ddf9ce587223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_conversion_to_spark'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919521e9-e627-4455-a687-1ddfd93b28e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad54d63-fc5b-4dfc-9d7a-609eb70d3e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0fc35-1b7b-4a0e-986e-a19c6da0d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.array_tasks import task_pivot_and_sort_arrays\n",
    "task_pivot_and_sort_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532645b-85ef-421e-8ee1-a7f30b83a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.array_tasks import task_diff_the_timestamp_arrays\n",
    "task_diff_the_timestamp_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de77202-1113-416c-bdce-d75479db1865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041391e9-06d3-4d18-9ead-426ae1322623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_pivot_and_sort'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5091c-7f32-461d-be98-70672b2296a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_timestamp_diff'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f010901-6f60-45fa-855f-1eb17e96acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c755d6-4493-46ed-bbc5-137ebc838bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b111045-53b0-49b1-a0f1-4cc1fb92c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5d6e-c904-43e6-ad48-bfc5a70ecc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_timestamps(timestamp_array, seconds_divisor):\n",
    "    return [int(x) for x in range(min(timestamp_array), max(timestamp_array) + seconds_divisor, seconds_divisor)]\n",
    "\n",
    "udf_get_all_timestamps = f.udf(get_all_timestamps, ArrayType(IntegerType()))\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def do_nans_exist(values_array):\n",
    "#    values_array = np.array([np.array(values_array)])\n",
    "#    mask = np.isnan(values_array)\n",
    "#    has_nan_0_or_1 = np.max([int(x) for x in mask[0]])\n",
    "#    return int(has_nan_0_or_1)\n",
    "#\n",
    "#udf_do_nans_exist = f.udf(do_nans_exist, IntegerType())\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def do_non_nans_exist(values_array):\n",
    "#    values_array = np.array([np.array(values_array)])\n",
    "#    mask = ~np.isnan(values_array)\n",
    "#    has_non_nan_0_or_1 = np.max([int(x) for x in mask[0]])\n",
    "#    return int(has_non_nan_0_or_1)\n",
    "#    \n",
    "#udf_do_non_nans_exist = f.udf(do_non_nans_exist, IntegerType())\n",
    "\n",
    "\n",
    "def count_nans_in_array(values_array):\n",
    "    values_array = np.array([np.array(values_array)])\n",
    "    mask = np.isnan(values_array)\n",
    "    nan_count = np.sum([int(x) for x in mask[0]])\n",
    "    return int(nan_count)\n",
    "    \n",
    "udf_count_nans_in_array = f.udf(count_nans_in_array, IntegerType())\n",
    "\n",
    "\n",
    "\n",
    "##https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "#def forward_fill(values_array):\n",
    "#    arr = np.array([values_array])\n",
    "#    mask = np.isnan(arr)\n",
    "#    idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "#    np.maximum.accumulate(idx, axis = 1, out = idx)\n",
    "#    arr[mask] = arr[np.nonzero(mask)[0], idx[mask]]\n",
    "#    to_return = list([float(x) for x in arr[0]])\n",
    "#    return to_return\n",
    "#\n",
    "#udf_forward_fill = f.udf(forward_fill, ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "def locate_nans(timestamp_array, timestamp_all_array, values_array):\n",
    "\n",
    "    # make sure we get an argsort in here later to ensure order of values is correct\n",
    "\n",
    "    ts = np.array(timestamp_array, dtype = np.uint64) # ??\n",
    "    ts_all = np.array(timestamp_all_array, dtype = np.uint64)  # we can probably make this smaller\n",
    "    v = np.array(values_array, dtype = np.float64)  # we can probably make this smaller\n",
    "    \n",
    "    pdf = pd.DataFrame({'timestamp' : ts, 'values' : v})\n",
    "    pdf_all = pd.DataFrame({'timestamp' : ts_all})\n",
    "\n",
    "    pdf_joined = (\n",
    "        pd.merge(\n",
    "            pdf_all,\n",
    "            pdf,\n",
    "            on = 'timestamp',\n",
    "            how = 'left',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    to_return = pdf_joined['values'].to_list()\n",
    "    \n",
    "    return to_return\n",
    "\n",
    "udf_locate_nans = f.udf(locate_nans, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140de41-d08c-43a6-bda2-c9449c586e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_get_all_timestamps\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_locate_nans\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_count_nans_in_array\n",
    "\n",
    "def task_find_full_day_nans(**config):\n",
    "\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = (\n",
    "        spark.read.parquet(config['directory_output'] + '/' + config['filename_timestamp_diff'])\n",
    "        .coalesce(config['n_processors_to_coalesce'])\n",
    "        .orderBy('date_post_shift')\n",
    "        .withColumn(\n",
    "            'timestamps_all',\n",
    "            udf_get_all_timestamps(f.col('sorted_timestamp_array'), f.lit(config['seconds_divisor']))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .withColumn(\n",
    "                item + '_and_nans',\n",
    "                udf_locate_nans(f.col('sorted_timestamp_array'), f.col('timestamps_all'), f.col('sorted_' + item + '_array'))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = (\n",
    "            sdf_arrays\n",
    "            .withColumn(\n",
    "                item + '_nan_count',\n",
    "                udf_count_nans_in_array(f.col(item + '_and_nans'))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sdf_arrays = sdf_arrays.withColumn('nan_count_full_day', f.col('return_nan_count'))\n",
    "    for item in config['list_data_columns']:\n",
    "        sdf_arrays = sdf_arrays.drop(item + '_nan_count')\n",
    "\n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1dc0fa-b1c1-4205-b576-9f940443a478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ca603-60fb-49bc-b082-9a9053483b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ef26a-af40-4c49-81a1-7ae40ff0c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import task_find_full_day_nans\n",
    "task_find_full_day_nans(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbba26-f489-4c11-9838-74786804c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead3234-e02e-474d-85fa-4f2bce5efdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d577b-a702-42d0-a599-6602e3368b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f3743-c749-4abc-8775-4da781b07f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99835ce-436d-4ce5-b037-dda94247ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0e2ab-7762-4425-970a-69c54c1372d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869a62e-b9be-47dd-af65-0ff624032788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QA #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24ce60-af98-4caa-91f4-0ef296219213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_qa = (\n",
    "    spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    .select('nan_count_full_day', 'date_post_shift')\n",
    "    .orderBy(f.col('nan_count_full_day').desc())\n",
    "    .withColumn('has_nans', f.col('nan_count_full_day') > 0)\n",
    "    .groupBy('has_nans').agg(f.count('date_post_shift').alias('number_of_days'))\n",
    ")\n",
    "sdf_qa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa66a-0472-4613-90d9-a9fc2c60344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_qa_counts = (\n",
    "    spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "    .select('nan_count_full_day', 'date_post_shift')\n",
    "    .orderBy(f.col('nan_count_full_day').desc())\n",
    ")\n",
    "sdf_qa_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4746b9-3cb9-4765-8eab-f5d521c77e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "nan_counts_per_day = [x['nan_count_full_day'] for x in sdf_qa_counts.select('nan_count_full_day').collect()]\n",
    "\n",
    "#plt.figure()\n",
    "#plt.hist(nan_counts_per_day)\n",
    "#plt.show()\n",
    "#plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot([nan_counts_per_day], widths=0.95)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ed94-8001-4b0a-bdfe-888a7c2d0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(nan_counts_per_day), np.median(nan_counts_per_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe322e8-0724-4d34-b6c7-29647d9c10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_qa_dates = sdf_qa_counts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97eb5c-455e-4996-931d-668cacb9df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_qa_dates['weekday'] = [x.weekday() for x in pdf_qa_dates['date_post_shift']]\n",
    "\n",
    "plt.figure()\n",
    "pdf_qa_dates.boxplot(by = 'weekday')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15a7d5-147e-4b43-839f-c393d83874fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## end QA #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f97ad-492d-4398-a441-6ec94ce07380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf951bc-53bf-497a-81a7-03e8a6c2c4e6",
   "metadata": {},
   "source": [
    "## Investigate consecutive NaNs for each day\n",
    "\n",
    "By identifying the maximum number of NaNs in a row for a given day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347f9e9-05dc-4339-a30b-510b7986f038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from forex.pre_training_data_prep.config import config\n",
    "from forex.pre_training_data_prep.tasks.spark.nan_related_tasks import udf_get_max_consecutive_NaNs\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + config['filename_full_day_nans'])\n",
    "for item in config['list_data_columns']:\n",
    "    sdf_arrays = sdf_arrays.withColumn('max_consec_nan_' + item, udf_get_max_consecutive_NaNs(f.col(item + '_and_nans')))\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "from itertools import groupby\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "def test_all_equality(*a_list):\n",
    "    g = groupby(a_list)\n",
    "    return next(g, True) and not next(g, False)\n",
    "\n",
    "udf_test_all_equality = f.udf(test_all_equality, BooleanType())\n",
    "\n",
    "\n",
    "columns_to_test = [f.col('max_consec_nan_' + x) for x in config['list_data_columns']]\n",
    "sdf_arrays = sdf_arrays.withColumn('is_consec_equal', udf_test_all_equality(*columns_to_test))\n",
    "\n",
    "columns_to_select = ['date_post_shift', 'nan_count_full_day']\n",
    "columns_to_select.extend(['max_consec_nan_' + x for x in config['list_data_columns']])\n",
    "columns_to_select.append('is_consec_equal')\n",
    "\n",
    "sdf_temp = sdf_arrays.select(*columns_to_select)\n",
    "\n",
    "#sdf_temp.show(5)\n",
    "\n",
    "# should all be true\n",
    "sdf_agg = (\n",
    "    sdf_temp.select('date_post_shift', 'is_consec_equal')\n",
    "    .groupBy('is_consec_equal')\n",
    "    .agg(f.count('date_post_shift').alias('count_column'))\n",
    ")\n",
    "#sdf_agg.show()\n",
    "\n",
    "\n",
    "sdf_arrays = (\n",
    "    sdf_arrays\n",
    "    .withColumn('max_daily_consec_nans', f.col('max_consec_nan_' + config['list_data_columns'][0]))\n",
    "    .drop('is_consec_equal')\n",
    "    .drop('sorted_timestamp_array', 'diff_sorted_timestamp_array')\n",
    ")\n",
    "\n",
    "for item in config['list_data_columns']:\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .drop(\n",
    "            'max_consec_nan_' + item,\n",
    "            'sorted_' + item + '_array',\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "#\n",
    "# length tests\n",
    "#\n",
    "sdf_arrays = sdf_arrays.withColumn('len_timestamps_all', f.array_size(f.col('timestamps_all')))\n",
    "for item in config['list_data_columns']:\n",
    "    sdf_arrays = sdf_arrays.withColumn('len_' + item, f.array_size(f.col(item + '_and_nans')))\n",
    "\n",
    "columns_to_test = [f.col('len_' + x) for x in config['list_data_columns']]\n",
    "columns_to_test.extend(['len_timestamps_all'])\n",
    "\n",
    "sdf_arrays = sdf_arrays.withColumn('is_length_equal', udf_test_all_equality(*columns_to_test))\n",
    "\n",
    "# True\n",
    "sdf_agg = (\n",
    "    sdf_arrays.select('date_post_shift', 'is_length_equal')\n",
    "    .groupBy('is_length_equal')\n",
    "    .agg(f.count('date_post_shift').alias('count_column'))\n",
    ")\n",
    "#sdf_agg.show()\n",
    "\n",
    "sdf_arrays = sdf_arrays.drop('is_length_equal', 'len_timestamps_all')\n",
    "for item in config['list_data_columns']:\n",
    "    sdf_arrays = sdf_arrays.drop('len_' + item)\n",
    "\n",
    "\n",
    "\n",
    "sdf_arrays.show(3)\n",
    "\n",
    "sdf_arrays.drop('nan_count_full_day', 'max_daily_consec_nans').write.mode('overwrite').parquet(config['directory_output'] + '/' + 'PLACEHOLDER.parquet')\n",
    "\n",
    "spark.stop()                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc800ee2-367e-41b0-8807-a7c91348049b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b2e71-a864-420b-b186-60f35b3f5aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0faab4-4a04-492e-b0a6-65fc6b16231a",
   "metadata": {},
   "source": [
    "## Trig functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9164fd6-415b-4a47-b6ae-4a36caa7ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "seasonal_forecasting_period = 60. * 60. * 24.\n",
    "seasonal_forecasting_frequency = (2. * np.pi) / seasonal_forecasting_period\n",
    "seasonal_forecasting_amplitude = 1.\n",
    "\n",
    "def sin_24_hours(timestamp_array_in_seconds):\n",
    "    result = seasonal_forecasting_amplitude * np.sin(seasonal_forecasting_frequency * np.array(timestamp_array_in_seconds))\n",
    "    return [float(x) for x in result]\n",
    "\n",
    "udf_sin_24_hours = f.udf(sin_24_hours, ArrayType(FloatType()))\n",
    "\n",
    "def cos_24_hours(timestamp_array_in_seconds):\n",
    "    result = seasonal_forecasting_amplitude * np.cos(seasonal_forecasting_frequency * np.array(timestamp_array_in_seconds))\n",
    "    return [float(x) for x in result]\n",
    "\n",
    "udf_cos_24_hours = f.udf(cos_24_hours, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d563433-bcd0-4793-8cda-ff23982777c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "from utilities.spark_session import get_spark_session\n",
    "\n",
    "def add_trig(**config):\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = spark.read.parquet(config['directory_output'] + '/' + 'PLACEHOLDER.parquet')\n",
    "\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn('sin_24', udf_sin_24_hours(f.col('timestamps_all')))\n",
    "        .withColumn('cos_24', udf_cos_24_hours(f.col('timestamps_all')))\n",
    "    )\n",
    "    \n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_post_trig'])\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7b2769-aa75-4d4b-91c8-62f556823133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 05:25:22 WARN Utils: Your hostname, emily-MS-7B96 resolves to a loopback address: 127.0.1.1; using 192.168.1.82 instead (on interface wlp5s0)\n",
      "25/03/13 05:25:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 05:25:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from forex.pre_training_data_prep.tasks.spark.trig import add_trig\n",
    "add_trig(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982ee2c-8eba-4002-aad4-41af203d26f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2595f74-5b58-4941-953f-084ce7a23d57",
   "metadata": {},
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c2a88-356c-41af-8d65-a34679103280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af045503-ad3a-4706-8656-e55a0dafc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "def do_we_have_enough_space_for_a_sliding_window(\n",
    "    the_list,\n",
    "    n_back,\n",
    "    n_forward,\n",
    "    offset,\n",
    "):\n",
    "    threshold = n_back + n_forward + offset    \n",
    "    return threshold <= len(the_list)\n",
    "\n",
    "udf_do_we_have_enough_space_for_a_sliding_window = f.udf(do_we_have_enough_space_for_a_sliding_window, BooleanType())\n",
    "\n",
    "sdf_arrays = (\n",
    "    sdf_arrays\n",
    "    .withColumn(\n",
    "        'is_long_enough',\n",
    "        udf_do_we_have_enough_space_for_a_sliding_window(\n",
    "            f.col('timestamps_all'),\n",
    "            f.lit(config['n_back']),\n",
    "            f.lit(config['n_forward']),\n",
    "            f.lit(config['offset']),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sdf_test = (\n",
    "    sdf_arrays.select('date_post_shift', 'is_long_enough')\n",
    "    .groupBy('is_long_enough')\n",
    "    .agg(f.count('date_post_shift').alias('count_column'))\n",
    ")\n",
    "\n",
    "#\n",
    "# I don't know why there is only one false result\n",
    "#\n",
    "\n",
    "sdf_arrays = (\n",
    "    sdf_arrays\n",
    "    .where(f.col('is_long_enough'))\n",
    "    .drop('is_long_enough')\n",
    ")\n",
    "\n",
    "sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ff0c5-4a31-4f7e-93a9-0f20600aff5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a7063-071d-4fbb-8910-df5678626cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from forex.pre_training_data_prep.tasks.spark.spark_sliding_window import do_sliding_window\n",
    "from forex.pre_training_data_prep.tasks.spark.spark_sliding_window import udf_make_sliding_window_float\n",
    "from forex.pre_training_data_prep.tasks.spark.spark_sliding_window import udf_make_sliding_window_int\n",
    "\n",
    "\n",
    "\n",
    "sdf_arrays = (\n",
    "    sdf_arrays\n",
    "    .withColumn(\n",
    "        'sw_timestamp',\n",
    "        udf_make_sliding_window_int(\n",
    "            f.col('timestamps_all'),\n",
    "            f.lit(config['n_back']),\n",
    "            f.lit(config['n_forward']),\n",
    "            f.lit(config['offset']),\n",
    "            f.lit(config['n_step']),\n",
    "        )\n",
    "    )\n",
    "    .drop('timestamps_all')\n",
    ")\n",
    "\n",
    "for item in config['list_data_columns']:\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn(\n",
    "            'sw_' + item,\n",
    "            udf_make_sliding_window_float(\n",
    "                f.col(item + '_and_nans'),\n",
    "                f.lit(config['n_back']),\n",
    "                f.lit(config['n_forward']),\n",
    "                f.lit(config['offset']),\n",
    "                f.lit(config['n_step']),\n",
    "            )\n",
    "        )\n",
    "        .drop(item + '_and_nans')\n",
    "    )\n",
    "\n",
    "for item in config['list_data_columns_no_scale']:\n",
    "    sdf_arrays = (\n",
    "        sdf_arrays\n",
    "        .withColumn(\n",
    "            'sw_' + item,\n",
    "            udf_make_sliding_window_float(\n",
    "                f.col(item),\n",
    "                f.lit(config['n_back']),\n",
    "                f.lit(config['n_forward']),\n",
    "                f.lit(config['offset']),\n",
    "                f.lit(config['n_step']),\n",
    "            )\n",
    "        )\n",
    "        .drop(item)\n",
    "    )\n",
    "\n",
    "\n",
    "sdf_arrays.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df498c-9109-4597-96cf-1c8bb45b1800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f405f8a-632b-48e3-babd-abe958bb4315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3861d81-dcec-4a53-93c7-6973bae33227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
