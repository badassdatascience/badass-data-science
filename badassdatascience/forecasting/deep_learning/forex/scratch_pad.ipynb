{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a696865-0400-4a18-a8dc-8f9927d6a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d4027f-2bc5-45d6-87a5-970646d17cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from forex.pre_training_data_prep.config import config\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "#from pyspark.sql.types import BooleanType, IntegerType, ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3034e5-9d8f-4af2-bf9d-0633beb3e782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'database_name': 'django',\n",
       " 'dag_id': 'NEW_prepare_forex_data',\n",
       " 'tz_name': 'US/Eastern',\n",
       " 'price_type_name': 'mid',\n",
       " 'instrument_name': 'EUR/USD',\n",
       " 'interval_name': 'Minute',\n",
       " 'retries_pull_forex_data': 1,\n",
       " 'retry_delay_minutes_pull_forex_data': 5,\n",
       " 'directory_output': '/home/emily/Desktop/projects/test/badass-data-science/badassdatascience/forecasting/deep_learning/forex/output',\n",
       " 'filename_candlesticks_query_results': 'candlesticks_query_results.parquet',\n",
       " 'filename_timezone_added': 'candlesticks_timezone_added.parquet',\n",
       " 'filename_offset': 'candlesticks_timezone_weekday_offset.parquet',\n",
       " 'filename_weekday_shift_merged': 'candlesticks_weekday_offset_merged.parquet',\n",
       " 'filename_shift_days_and_hours_as_needed': 'candlesticks_shifted_as_needed.parquet',\n",
       " 'filename_finalized_pandas': 'candlesticks_finalized_pandas.parquet',\n",
       " 'filename_conversion_to_spark': 'spark_converted.parquet',\n",
       " 'filename_pivot_and_sort': 'spark_pivot_and_sort.parquet',\n",
       " 'filename_timestamp_diff': 'spark_timestamp_diff.parquet',\n",
       " 'spark_config': [('spark.executor.memory', '75g'),\n",
       "  ('spark.executor.cores', '20'),\n",
       "  ('spark.cores.max', '20'),\n",
       "  ('spark.driver.memory', '75g'),\n",
       "  ('spark.sql.execution.arrow.pyspark.enabled', 'true')],\n",
       " 'n_processors_to_coalesce': 20,\n",
       " 'seconds_divisor': 60}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f488-156a-4d7b-ae34-f0a00aadc11b",
   "metadata": {},
   "source": [
    "## Get previous Pandas dataframe and put it into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3a293b-d431-40d2-9702-dc282eba073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 08:21:33 WARN Utils: Your hostname, emily-MS-7B96 resolves to a loopback address: 127.0.1.1; using 192.168.1.82 instead (on interface wlp5s0)\n",
      "25/03/10 08:21:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 08:21:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/emily/venvs/ml/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  [UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION] uint8 is not supported in conversion to Arrow.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "25/03/10 08:27:21 WARN TaskSetManager: Stage 0 contains a task of very large size (18671 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utilities.spark_session import get_spark_session\n",
    "from utilities.spark_session import load_pandas_df_parquet_into_spark_df\n",
    "\n",
    "spark = get_spark_session(config['spark_config'])\n",
    "\n",
    "sdf_arrays = load_pandas_df_parquet_into_spark_df(\n",
    "    config['directory_output'] + '/' + config['filename_finalized_pandas'],\n",
    "    spark,\n",
    "    truncate_to_row_number = None, #10,\n",
    "    n_processors_to_coalesce = config['n_processors_to_coalesce'],\n",
    ")\n",
    "\n",
    "sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_conversion_to_spark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55387e24-06da-4aa1-82e1-e7c19bb86410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 08:27:26 WARN TaskSetManager: Stage 1 contains a task of very large size (18671 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/10 08:27:30 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 20): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-------+-------+------+-------------------+----------+-------+---------------+---------------------+--------------------+--------------------+------------------+\n",
      "| timestamp|      o|      l|      h|      c|volume|        datetime_tz|weekday_tz|hour_tz|weekday_shifted|original_date_shifted|              Return|          Volatility|          lhc_mean|\n",
      "+----------+-------+-------+-------+-------+------+-------------------+----------+-------+---------------+---------------------+--------------------+--------------------+------------------+\n",
      "|1220599980|1.42861|1.42825|1.42876| 1.4284|    51|2008-09-05 00:33:00|         4|      3|              4|           2008-09-05|-2.10000000000043...|5.100000000000104E-4|           1.42847|\n",
      "|1220600040|1.42845|1.42823|1.42863|1.42832|    57|2008-09-05 00:34:00|         4|      3|              4|           2008-09-05|-1.29999999999963...|3.999999999999559...|1.4283933333333334|\n",
      "|1220600100|1.42837|1.42827|1.42857|1.42844|    26|2008-09-05 00:35:00|         4|      3|              4|           2008-09-05| 7.00000000000145E-5|2.999999999999669...|1.4284266666666667|\n",
      "|1220600160|1.42844| 1.4279|1.42852|1.42847|    64|2008-09-05 00:36:00|         4|      3|              4|           2008-09-05|2.999999999997449E-5|6.200000000000649E-4|1.4282966666666665|\n",
      "|1220600220|1.42844|1.42821|1.42844|1.42827|    33|2008-09-05 00:37:00|         4|      3|              4|           2008-09-05|-1.70000000000003...|2.299999999999524...|1.4283066666666666|\n",
      "+----------+-------+-------+-------+-------+------+-------------------+----------+-------+---------------+---------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_arrays.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8461e3f-e59e-49fd-ada9-67c88443003b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a29b58-d7ea-479d-b384-ddf9ce587223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919521e9-e627-4455-a687-1ddfd93b28e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439a3d0-26d5-4fcc-94b8-9d5414db99d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdbc941-5a94-4484-99ab-bdde6e1be946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c30c92-0ddf-40da-b7a8-2785b36680eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_an_array(the_array, seconds_divisor):\n",
    "    return [int((y - x) / seconds_divisor) for x, y in zip(the_array[0:-1], the_array[1:])]\n",
    "\n",
    "udf_difference_an_array = f.udf(difference_an_array, ArrayType(IntegerType()))\n",
    "\n",
    "def argsort_an_array(the_array):\n",
    "    the_sort_order = np.argsort(np.array(the_array))\n",
    "    return [int(x) for x in the_sort_order]\n",
    "\n",
    "udf_argsort_an_array = f.udf(argsort_an_array, ArrayType(IntegerType()))\n",
    "\n",
    "def apply_argsort_integer(the_array, argsort_array):\n",
    "    return [int(x) for x in np.array(the_array)[argsort_array]]\n",
    "\n",
    "def apply_argsort_float(the_array, argsort_array):\n",
    "    return [float(x) for x in np.array(the_array)[argsort_array]]\n",
    "\n",
    "udf_apply_argsort_integer = f.udf(apply_argsort_integer, ArrayType(IntegerType()))\n",
    "udf_apply_argsort_float = f.udf(apply_argsort_float, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3cbba97-317f-4ce4-957e-5d8f71e800f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_pivot_and_sort_arrays(**config):\n",
    "\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = (\n",
    "\n",
    "        #\n",
    "        # load\n",
    "        #\n",
    "        spark.read.parquet(config['directory_output'] + '/' + config['filename_conversion_to_spark'])\n",
    "        \n",
    "        #\n",
    "        # pivot\n",
    "        #\n",
    "        .select('original_date_shifted', 'timestamp', 'Return', 'Volatility', 'lhc_mean', 'volume')\n",
    "        .withColumnRenamed('original_date_shifted', 'date_post_shift')\n",
    "        .orderBy('timestamp')\n",
    "        .groupBy('date_post_shift')\n",
    "        .agg(\n",
    "            f.collect_list('timestamp').alias('timestamp_array'),\n",
    "            f.collect_list('Return').alias('return_array'),\n",
    "            f.collect_list('Volatility').alias('volatility_array'),\n",
    "            f.collect_list('lhc_mean').alias('lhc_mean_array'),\n",
    "            f.collect_list('volume').alias('volume_array'),\n",
    "        )\n",
    "\n",
    "        #\n",
    "        # ensure proper sorting (probably not necessary but insurance)\n",
    "        #\n",
    "        .withColumn(\n",
    "            'timestamp_argsort',\n",
    "            udf_argsort_an_array(f.col('timestamp_array'))\n",
    "        )\n",
    "        .withColumn('sorted_timestamp_array', udf_apply_argsort_integer(f.col('timestamp_array'), f.col('timestamp_argsort')))\n",
    "        .withColumn('sorted_return_array', udf_apply_argsort_float(f.col('return_array'), f.col('timestamp_argsort')))\n",
    "        .withColumn('sorted_volatility_array', udf_apply_argsort_float(f.col('volatility_array'), f.col('timestamp_argsort')))\n",
    "        .withColumn('sorted_lhc_mean_array', udf_apply_argsort_float(f.col('lhc_mean_array'), f.col('timestamp_argsort')))\n",
    "        .withColumn('sorted_volume_array', udf_apply_argsort_float(f.col('volume_array'), f.col('timestamp_argsort')))\n",
    "\n",
    "        .drop('timestamp_array', 'return_array', 'volatility_array', 'lhc_mean_array', 'volume_array')\n",
    "        .orderBy('date_post_shift')\n",
    "    )\n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_pivot_and_sort'])\n",
    "    spark.stop()\n",
    "\n",
    "def task_diff_the_timestamp_arrays(**config):\n",
    "    spark = get_spark_session(config['spark_config'])\n",
    "    sdf_arrays = (\n",
    "        spark.read.parquet(config['directory_output'] + '/' + config['filename_pivot_and_sort'])\n",
    "        .withColumn('seconds_divisor', f.lit(config['seconds_divisor']))\n",
    "        .withColumn(\n",
    "            'diff_sorted_timestamp_array',\n",
    "            udf_difference_an_array(\n",
    "                f.col('sorted_timestamp_array'),\n",
    "                f.col('seconds_divisor'),\n",
    "            )\n",
    "        )\n",
    "        .drop('seconds_divisor')\n",
    "        .orderBy('date_post_shift')\n",
    "    )\n",
    "    sdf_arrays.write.mode('overwrite').parquet(config['directory_output'] + '/' + config['filename_timestamp_diff'])\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e0fc35-1b7b-4a0e-986e-a19c6da0d8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "task_pivot_and_sort_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8532645b-85ef-421e-8ee1-a7f30b83a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "task_diff_the_timestamp_arrays(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041391e9-06d3-4d18-9ead-426ae1322623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5091c-7f32-461d-be98-70672b2296a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
